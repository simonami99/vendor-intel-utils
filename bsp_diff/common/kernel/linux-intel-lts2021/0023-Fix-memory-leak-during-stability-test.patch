From fdddd434cb799b6931a7bd2e615354fb1e94136d Mon Sep 17 00:00:00 2001
From: "Badrappan, Jeevaka" <jeevaka.badrappan@intel.com>
Date: Sat, 5 Aug 2023 14:45:56 +0530
Subject: [PATCH] Fix memory leak during stability test

Memory available kept reducing during stability test.

This reverts following commits(internal kernel tree) to address running
out of memory issue seen during aic-cloud start..stop cycle.
- Manage vm_bind_list under vm->root_obj->resv locking
- Close vm_unbind vs evict race
- drm/i915/atsm: FIXME: workaround a random GPU hang

Tracked-On: OAM-111528
Signed-off-by: Badrappan, Jeevaka <jeevaka.badrappan@intel.com>
---
 drivers/gpu/drm/i915/display/intel_dpt.c      |   7 +-
 drivers/gpu/drm/i915/display/intel_fb_pin.c   |   2 +-
 drivers/gpu/drm/i915/gem/i915_gem_domain.c    |   8 +-
 drivers/gpu/drm/i915/gem/i915_gem_domain.h    |   4 +-
 .../gpu/drm/i915/gem/i915_gem_execbuffer.c    | 232 +++++++++++-------
 drivers/gpu/drm/i915/gem/i915_gem_object.c    |   2 +
 drivers/gpu/drm/i915/gem/i915_gem_object.h    |  26 ++
 .../gpu/drm/i915/gem/i915_gem_object_types.h  |   4 +
 drivers/gpu/drm/i915/gem/i915_gem_pages.c     |   4 -
 drivers/gpu/drm/i915/gem/i915_gem_shrinker.c  |  50 ++--
 drivers/gpu/drm/i915/gem/i915_gem_vm_bind.h   |  24 ++
 .../drm/i915/gem/i915_gem_vm_bind_object.c    | 166 +++++--------
 .../i915/gem/selftests/i915_gem_execbuffer.c  |  17 +-
 drivers/gpu/drm/i915/gt/intel_ggtt.c          |  19 +-
 drivers/gpu/drm/i915/gt/intel_gtt.c           |  54 +++-
 drivers/gpu/drm/i915/gt/intel_gtt.h           |  33 ++-
 drivers/gpu/drm/i915/gt/intel_pagefault.c     |  32 ++-
 drivers/gpu/drm/i915/gt/intel_ppgtt.c         |   1 +
 drivers/gpu/drm/i915/i915_debugger.c          |   2 +-
 drivers/gpu/drm/i915/i915_gem.c               |  90 ++++---
 drivers/gpu/drm/i915/i915_gem_ww.c            |  26 +-
 drivers/gpu/drm/i915/i915_gem_ww.h            |   6 +-
 drivers/gpu/drm/i915/i915_params.c            |  15 ++
 drivers/gpu/drm/i915/i915_params.h            |   1 +
 drivers/gpu/drm/i915/i915_sw_fence_work.c     |   2 -
 drivers/gpu/drm/i915/i915_vma.c               |  75 +++---
 drivers/gpu/drm/i915/i915_vma_types.h         |   6 +-
 drivers/gpu/drm/i915/intel_memory_region.c    |  30 ++-
 drivers/gpu/drm/i915/selftests/mock_gtt.c     |   7 +-
 29 files changed, 588 insertions(+), 357 deletions(-)

diff --git a/drivers/gpu/drm/i915/display/intel_dpt.c b/drivers/gpu/drm/i915/display/intel_dpt.c
index e1929f13d43e..d8488c61cfaf 100644
--- a/drivers/gpu/drm/i915/display/intel_dpt.c
+++ b/drivers/gpu/drm/i915/display/intel_dpt.c
@@ -236,7 +236,6 @@ intel_dpt_create(struct intel_framebuffer *fb)
 	struct i915_address_space *vm;
 	struct i915_dpt *dpt;
 	size_t size;
-	int err;
 
 	if (intel_fb_needs_pot_stride_remap(fb))
 		size = intel_remapped_info_size(&fb->remapped_view.gtt.remapped);
@@ -271,11 +270,7 @@ intel_dpt_create(struct intel_framebuffer *fb)
 	vm->total = (size / sizeof(gen8_pte_t)) * I915_GTT_PAGE_SIZE;
 	vm->is_dpt = true;
 
-	err = i915_address_space_init(vm, VM_CLASS_DPT);
-	if (err) {
-		i915_gem_object_put(dpt_obj);
-		return ERR_PTR(err);
-	}
+	i915_address_space_init(vm, VM_CLASS_DPT);
 
 	vm->insert_page = dpt_insert_page;
 	vm->clear_range = dpt_clear_range;
diff --git a/drivers/gpu/drm/i915/display/intel_fb_pin.c b/drivers/gpu/drm/i915/display/intel_fb_pin.c
index 05204a04da90..85caa76451d3 100644
--- a/drivers/gpu/drm/i915/display/intel_fb_pin.c
+++ b/drivers/gpu/drm/i915/display/intel_fb_pin.c
@@ -39,7 +39,7 @@ intel_pin_fb_obj_dpt(struct drm_framebuffer *fb,
 
 	ret = i915_gem_object_lock_interruptible(obj, NULL);
 	if (!ret) {
-		ret = i915_gem_object_set_cache_level(obj, I915_CACHE_NONE);
+		ret = i915_gem_object_set_cache_level(obj, NULL, I915_CACHE_NONE);
 		i915_gem_object_unlock(obj);
 	}
 	if (ret) {
diff --git a/drivers/gpu/drm/i915/gem/i915_gem_domain.c b/drivers/gpu/drm/i915/gem/i915_gem_domain.c
index f016e484e226..af5b370f46a5 100644
--- a/drivers/gpu/drm/i915/gem/i915_gem_domain.c
+++ b/drivers/gpu/drm/i915/gem/i915_gem_domain.c
@@ -223,6 +223,7 @@ i915_gem_object_set_to_gtt_domain(struct drm_i915_gem_object *obj, bool write)
 /**
  * Changes the cache-level of an object across all VMA.
  * @obj: object to act on
+ * @ww: locking context
  * @cache_level: new cache level to set for the object
  *
  * After this function returns, the object will be in the new cache-level
@@ -236,6 +237,7 @@ i915_gem_object_set_to_gtt_domain(struct drm_i915_gem_object *obj, bool write)
  * that all direct access to the scanout remains coherent.
  */
 int i915_gem_object_set_cache_level(struct drm_i915_gem_object *obj,
+				    struct i915_gem_ww_ctx *ww,
 				    enum i915_cache_level cache_level)
 {
 	int ret;
@@ -258,7 +260,7 @@ int i915_gem_object_set_cache_level(struct drm_i915_gem_object *obj,
 	obj->cache_dirty = true;
 
 	/* The cache-level will be applied when each vma is rebound. */
-	return i915_gem_object_unbind(obj, NULL,
+	return i915_gem_object_unbind(obj, ww,
 				      I915_GEM_OBJECT_UNBIND_ACTIVE |
 				      I915_GEM_OBJECT_UNBIND_BARRIER);
 }
@@ -346,7 +348,7 @@ int i915_gem_set_caching_ioctl(struct drm_device *dev, void *data,
 	if (ret)
 		goto out;
 
-	ret = i915_gem_object_set_cache_level(obj, level);
+	ret = i915_gem_object_set_cache_level(obj, NULL, level);
 	i915_gem_object_unlock(obj);
 
 out:
@@ -386,7 +388,7 @@ i915_gem_object_pin_to_display_plane(struct drm_i915_gem_object *obj,
 	 * of uncaching, which would allow us to flush all the LLC-cached data
 	 * with that bit in the PTE to main memory with just one PIPE_CONTROL.
 	 */
-	ret = i915_gem_object_set_cache_level(obj,
+	ret = i915_gem_object_set_cache_level(obj, NULL,
 					      HAS_WT(i915) ?
 					      I915_CACHE_WT : I915_CACHE_NONE);
 	if (ret)
diff --git a/drivers/gpu/drm/i915/gem/i915_gem_domain.h b/drivers/gpu/drm/i915/gem/i915_gem_domain.h
index 9622df962bfc..f28f2b658127 100644
--- a/drivers/gpu/drm/i915/gem/i915_gem_domain.h
+++ b/drivers/gpu/drm/i915/gem/i915_gem_domain.h
@@ -6,10 +6,12 @@
 #ifndef __I915_GEM_DOMAIN_H__
 #define __I915_GEM_DOMAIN_H__
 
-struct drm_i915_gem_object;
 enum i915_cache_level;
+struct drm_i915_gem_object;
+struct i915_gem_ww_ctx;
 
 int i915_gem_object_set_cache_level(struct drm_i915_gem_object *obj,
+				    struct i915_gem_ww_ctx *ww,
 				    enum i915_cache_level cache_level);
 
 #endif /* __I915_GEM_DOMAIN_H__ */
diff --git a/drivers/gpu/drm/i915/gem/i915_gem_execbuffer.c b/drivers/gpu/drm/i915/gem/i915_gem_execbuffer.c
index 61da0c2c089f..cd8b30fd97d1 100644
--- a/drivers/gpu/drm/i915/gem/i915_gem_execbuffer.c
+++ b/drivers/gpu/drm/i915/gem/i915_gem_execbuffer.c
@@ -68,9 +68,9 @@ enum {
 #define __EXEC_OBJECT_INTERNAL_FLAGS	(~0u << 27) /* all of the above + */
 #define __EXEC_OBJECT_RESERVED (__EXEC_OBJECT_HAS_PIN | __EXEC_OBJECT_HAS_FENCE)
 
-#define __EXEC_LOCK_PERSISTENT		BIT_ULL(34)
-#define __EXEC_HAS_RELOC		BIT_ULL(33)
-#define __EXEC_ENGINE_PINNED		BIT_ULL(32)
+#define __EXEC_HAS_RELOC		BIT_ULL(34)
+#define __EXEC_ENGINE_PINNED		BIT_ULL(33)
+#define __EXEC_PERSIST_HAS_PIN  	BIT_ULL(32)
 #define __EXEC_INTERNAL_FLAGS		GENMASK_ULL(34, 32)
 #define UPDATE				PIN_OFFSET_FIXED
 
@@ -583,16 +583,16 @@ static int __eb_persistent_vmas_move_to_active(struct i915_execbuffer *eb,
 					       struct i915_request *rq)
 {
 	struct i915_address_space *vm = eb->context->vm;
+	struct i915_vma *vma;
+	int err;
 
-	if (eb->args->flags & __EXEC_LOCK_PERSISTENT) {
-		struct i915_vma *vma;
-		int err;
+	assert_vm_bind_held(vm);
+	assert_vm_priv_held(vm);
 
-		list_for_each_entry(vma, &vm->vm_bind_list, vm_bind_link) {
-			err = __i915_request_await_bind(rq, vma);
-			if (err)
-				return err;
-		}
+	list_for_each_entry(vma, &vm->vm_bind_list, vm_bind_link) {
+		err = __i915_request_await_bind(rq, vma);
+		if (err)
+			return err;
 	}
 
 	return i915_vm_move_to_active(vm, rq->context, rq);
@@ -835,6 +835,30 @@ static int eb_select_context(struct i915_execbuffer *eb)
 	return 0;
 }
 
+static struct i915_vma *
+eb_check_for_persistent_vma(struct i915_execbuffer *eb,
+			    struct drm_i915_gem_exec_object2 *entry)
+{
+	struct i915_address_space *vm = eb->context->vm;
+	struct i915_vma *vma;
+	u64 va;
+
+	assert_vm_bind_held(vm);
+
+	if (!test_bit(I915_VM_HAS_PERSISTENT_BINDS, &vm->flags))
+		return NULL;
+
+	va = intel_noncanonical_addr(INTEL_PPGTT_MSB(vm->i915),
+				     entry->offset & PIN_OFFSET_MASK);
+	vma = i915_gem_vm_bind_lookup_vma(vm, va);
+	if (!vma)
+		return NULL;
+
+	i915_vma_get(vma);
+
+	return vma;
+}
+
 static int __eb_add_lut(struct i915_execbuffer *eb,
 			u32 handle, struct i915_vma *vma)
 {
@@ -884,7 +908,6 @@ static int __eb_add_lut(struct i915_execbuffer *eb,
 	if (unlikely(err))
 		goto err_lut;
 
-	set_bit(I915_VMA_HAS_LUT_BIT, __i915_vma_flags(vma));
 	return 0;
 
 err_lut:
@@ -895,38 +918,14 @@ static int __eb_add_lut(struct i915_execbuffer *eb,
 	return err;
 }
 
-static struct i915_vma *
-eb_check_for_persistent_vma(struct i915_execbuffer *eb,
-			    const struct drm_i915_gem_exec_object2 *entry)
+static struct i915_vma *eb_lookup_vma(struct i915_execbuffer *eb, u32 handle)
 {
 	struct i915_address_space *vm = eb->context->vm;
-	struct i915_vma *vma;
-	u64 va;
-
-	if (!test_bit(I915_VM_HAS_PERSISTENT_BINDS, &vm->flags))
-		return NULL;
-
-	va = intel_noncanonical_addr(INTEL_PPGTT_MSB(vm->i915),
-				     entry->offset & PIN_OFFSET_MASK);
 
-	i915_gem_vm_bind_lock(vm);
-	vma = i915_gem_vm_bind_lookup_vma(vm, va);
-	if (vma) {
-		if (entry->handle)
-			__eb_add_lut(eb, entry->handle, vma);
-		vma = i915_vma_get(vma);
+	if (i915_gem_context_is_lr(eb->gem_context)) {
+		drm_dbg(&eb->i915->drm, "Long Running Ctx: Non VM_BIND vma.\n");
+		return ERR_PTR(-EINVAL);
 	}
-	i915_gem_vm_bind_unlock(vm);
-
-	return vma;
-}
-
-static struct i915_vma *
-eb_lookup_vma(struct i915_execbuffer *eb,
-	      const struct drm_i915_gem_exec_object2 *entry)
-{
-	struct i915_address_space *vm = eb->context->vm;
-	u32 handle = entry->handle;
 
 	do {
 		struct drm_i915_gem_object *obj;
@@ -946,10 +945,6 @@ eb_lookup_vma(struct i915_execbuffer *eb,
 		if (signal_pending(current))
 			return ERR_PTR(-ERESTARTSYS);
 
-		vma = eb_check_for_persistent_vma(eb, entry);
-		if (vma)
-			return vma;
-
 		obj = i915_gem_object_lookup(eb->file, handle);
 		if (unlikely(!obj))
 			return ERR_PTR(-ENOENT);
@@ -1007,9 +1002,13 @@ static int eb_lookup_vmas(struct i915_execbuffer *eb)
 
 	for (i = 0; i < eb->buffer_count; i++) {
 		struct drm_i915_gem_exec_object2 *entry = &eb->exec[i];
+		u32 handle = entry->handle;
 		struct i915_vma *vma;
 
-		vma = eb_lookup_vma(eb, entry);
+		vma = eb_check_for_persistent_vma(eb, entry);
+		if (!vma)
+			vma = eb_lookup_vma(eb, handle);
+
 		if (IS_ERR(vma)) {
 			err = PTR_ERR(vma);
 			goto err;
@@ -1036,32 +1035,28 @@ static int eb_lookup_vmas(struct i915_execbuffer *eb)
 static int eb_lock_persistent_vmas(struct i915_execbuffer *eb)
 {
 	struct i915_address_space *vm = eb->context->vm;
-	struct i915_vma *vma, *vn;
+	struct i915_vma *vma;
 	int err;
 
-	if (!test_bit(I915_VM_HAS_PERSISTENT_BINDS, &vm->flags))
-		return 0;
-
-	err = i915_vm_lock_objects(vm, &eb->ww);
+	err = i915_gem_vm_priv_lock(eb->context->vm, &eb->ww);
 	if (err)
 		return err;
 
-	if (!atomic_read(&vm->open))
-		return -ENOENT;
-
-	list_for_each_entry_safe(vma, vn, &vm->vm_bind_list, vm_bind_link) {
-		if (i915_vma_is_bound(vma, PIN_USER) &&
-		    i915_vma_is_bind_complete(vma)) {
-			list_move(&vma->vm_bind_link, &vm->vm_bound_list);
-			continue;
+	if (eb->i915->params.enable_non_private_objects) {
+		list_for_each_entry(vma, &vm->non_priv_vm_bind_list,
+				    non_priv_vm_bind_link) {
+			err = i915_gem_object_lock(vma->obj, &eb->ww);
+			if (err)
+				return err;
+		}
+	} else {
+		list_for_each_entry(vma, &vm->vm_bind_list, vm_bind_link) {
+			err = i915_gem_object_lock(vma->obj, &eb->ww);
+			if (err)
+				return err;
 		}
-
-		err = i915_gem_object_lock(vma->obj, &eb->ww);
-		if (err)
-			return err;
 	}
 
-	eb->args->flags |= __EXEC_LOCK_PERSISTENT;
 	return 0;
 }
 
@@ -1075,7 +1070,11 @@ static int eb_lock_vmas(struct i915_execbuffer *eb)
 		return err;
 
 	for (i = 0; i < eb->buffer_count; i++) {
-		struct i915_vma *vma = eb->vma[i].vma;
+		struct eb_vma *ev = &eb->vma[i];
+		struct i915_vma *vma = ev->vma;
+
+		if (vma->obj->base.resv == vma->vm->root_obj->base.resv)
+			continue;
 
 		err = i915_gem_object_lock(vma->obj, &eb->ww);
 		if (err)
@@ -1089,21 +1088,28 @@ static int eb_validate_persistent_vmas(struct i915_execbuffer *eb)
 {
 	struct i915_address_space *vm = eb->context->vm;
 	struct i915_vma *vma;
-	int err;
 
-	if (!(eb->args->flags & __EXEC_LOCK_PERSISTENT))
+	assert_vm_bind_held(vm);
+	assert_vm_priv_held(vm);
+
+	if (list_empty(&vm->vm_bind_list))
 		return 0;
 
 	list_for_each_entry(vma, &vm->vm_bind_list, vm_bind_link) {
-		u64 pin_flags = vma->node.start | PIN_OFFSET_FIXED | PIN_USER;
+		u64 pin_flags = vma->start | PIN_OFFSET_FIXED | PIN_USER;
+		int err;
 
 		err = i915_vma_pin_ww(vma, &eb->ww, 0, 0, pin_flags);
-		if (err)
-			return err;
+		if (!err)
+			continue;
 
-		__i915_vma_unpin(vma);
+		list_for_each_entry_continue_reverse(vma, &vm->vm_bind_list, vm_bind_link)
+			__i915_vma_unpin(vma);
+
+		return err;
 	}
 
+	eb->args->flags |= __EXEC_PERSIST_HAS_PIN;
 	return 0;
 }
 
@@ -1152,12 +1158,29 @@ static int eb_mem_fence_init(struct i915_execbuffer *eb)
 	return 0;
 }
 
+static void eb_scoop_persistent_unbound_vmas(struct i915_address_space *vm)
+{
+	struct i915_vma *vma, *vn;
+
+	if (list_empty(&vm->vm_rebind_list))
+		return;
+
+	spin_lock(&vm->vm_rebind_lock);
+	list_for_each_entry_safe(vma, vn, &vm->vm_rebind_list, vm_rebind_link) {
+		list_del_init(&vma->vm_rebind_link);
+		if (!list_empty(&vma->vm_bind_link))
+			list_move_tail(&vma->vm_bind_link, &vm->vm_bind_list);
+	}
+	spin_unlock(&vm->vm_rebind_lock);
+}
+
 static int eb_validate_vmas(struct i915_execbuffer *eb)
 {
 	unsigned int i;
 	int err;
 
 	INIT_LIST_HEAD(&eb->unbound);
+	eb_scoop_persistent_unbound_vmas(eb->context->vm);
 
 	err = eb_lock_vmas(eb);
 	if (err)
@@ -1238,6 +1261,30 @@ eb_get_vma(const struct i915_execbuffer *eb, unsigned long handle)
 	}
 }
 
+static void eb_release_persistent_vmas(struct i915_execbuffer *eb, bool final)
+{
+	struct i915_address_space *vm = eb->context->vm;
+	struct i915_vma *vma, *vn;
+
+	assert_vm_bind_held(vm);
+
+	if (!(eb->args->flags & __EXEC_PERSIST_HAS_PIN))
+		return;
+
+	assert_vm_priv_held(vm);
+
+	list_for_each_entry(vma, &vm->vm_bind_list, vm_bind_link)
+		__i915_vma_unpin(vma);
+
+	eb->args->flags &= ~__EXEC_PERSIST_HAS_PIN;
+	if (!final)
+		return;
+
+	list_for_each_entry_safe(vma, vn, &vm->vm_bind_list, vm_bind_link)
+		if (i915_vma_is_bind_complete(vma))
+			list_move_tail(&vma->vm_bind_link, &vm->vm_bound_list);
+}
+
 static void eb_release_vmas(struct i915_execbuffer *eb, bool final)
 {
 	const unsigned int count = eb->buffer_count;
@@ -1256,6 +1303,7 @@ static void eb_release_vmas(struct i915_execbuffer *eb, bool final)
 			i915_vma_put(vma);
 	}
 
+	eb_release_persistent_vmas(eb, final);
 	eb_unpin_engine(eb);
 }
 
@@ -2272,6 +2320,7 @@ static noinline int eb_relocate_parse_slow(struct i915_execbuffer *eb)
 	eb_release_vmas(eb, false);
 
 	i915_gem_ww_ctx_fini(&eb->ww);
+	i915_gem_vm_bind_unlock(eb->context->vm);
 
 	/*
 	 * We take 3 passes through the slowpatch.
@@ -2296,10 +2345,16 @@ static noinline int eb_relocate_parse_slow(struct i915_execbuffer *eb)
 		cond_resched();
 		err = 0;
 	}
-	if (err)
-		goto out;
+
+	/*
+	 * FIXME: Should lock interruptible, but needs to return with the
+	 * lock held.
+	 */
+	i915_gem_vm_bind_lock(eb->context->vm);
 
 	i915_gem_ww_ctx_init(&eb->ww, true);
+	if (err)
+		goto out;
 
 	/* reacquire the objects */
 repeat_validate:
@@ -2359,8 +2414,6 @@ static noinline int eb_relocate_parse_slow(struct i915_execbuffer *eb)
 	if (err == -EAGAIN)
 		goto repeat;
 
-	i915_gem_ww_ctx_fini(&eb->ww);
-
 out:
 	for (i = 0; i < have_copy; i++) {
 		const struct drm_i915_gem_exec_object2 *entry = &eb->exec[i];
@@ -4047,10 +4100,12 @@ i915_gem_do_execbuffer(struct drm_device *dev,
 	if (unlikely(err))
 		goto err_sfence;
 
+	i915_gem_vm_bind_lock(eb.context->vm);
+
 	err = eb_lookup_vmas(&eb);
 	if (err) {
 		eb_release_vmas(&eb, true);
-		goto err_exit;
+		goto err_vm_bind_unlock;
 	}
 
 	i915_gem_ww_ctx_init(&eb.ww, true);
@@ -4154,7 +4209,8 @@ i915_gem_do_execbuffer(struct drm_device *dev,
 		intel_gt_buffer_pool_put(eb.reloc_pool);
 	if (eb.reloc_context)
 		intel_context_put(eb.reloc_context);
-err_exit:
+err_vm_bind_unlock:
+	i915_gem_vm_bind_unlock(eb.context->vm);
 	eb_exit(&eb);
 err_sfence:
 	kfree(sfence);
@@ -4276,15 +4332,16 @@ end:;
 static int persistent_vmas_move_to_active_sync(struct i915_execbuffer *eb)
 {
 	struct i915_address_space *vm = eb->context->vm;
-	struct i915_vma *vma, *vn;
-	int err;
+	struct i915_vma *vma;
+	int err = 0;
 
-	list_for_each_entry_safe(vma, vn, &vm->vm_bind_list, vm_bind_link) {
+	assert_vm_bind_held(vm);
+	assert_vm_priv_held(vm);
+
+	list_for_each_entry(vma, &vm->vm_bind_list, vm_bind_link) {
 		err = i915_vma_wait_for_bind(vma);
 		if (err)
 			return err;
-
-		list_move_tail(&vma->vm_bind_link, &vm->vm_bound_list);
 	}
 
 	return i915_vm_move_to_active(vm, eb->context, NULL);
@@ -4318,8 +4375,9 @@ static int revalidate_transaction(struct i915_execbuffer *eb)
 	 */
 	tl = intel_context_timeline_lock(ce);
 	if (IS_ERR(tl)) {
+		err = PTR_ERR(tl);
 		kfree(sfence);
-		return PTR_ERR(tl);
+		return err;
 	}
 
 	/* If context is retired, abort */
@@ -4373,15 +4431,16 @@ static int revalidate_transaction(struct i915_execbuffer *eb)
  */
 static void i915_gem_exec_revalidate(struct intel_context *ce)
 {
-	struct drm_i915_gem_execbuffer2 args = {};
-	struct i915_execbuffer eb = {};
+	struct drm_i915_gem_execbuffer2 args = {0};
+	struct i915_execbuffer eb = {0};
 	int err;
 
 	/* Only populate parts of @eb we actually use. */
 	eb.context = ce;
-	eb.i915 = ce->engine->i915;
+	eb.i915 = eb.context->engine->i915;
 	eb.args = &args;
 
+	i915_gem_vm_bind_lock(ce->vm);
 retry:
 	i915_gem_ww_ctx_init(&eb.ww, true);
 revalidate:
@@ -4391,11 +4450,14 @@ static void i915_gem_exec_revalidate(struct intel_context *ce)
 		if (!err)
 			goto revalidate;
 	}
+	eb_release_persistent_vmas(&eb, true);
 	i915_gem_ww_ctx_fini(&eb.ww);
 
 	if (err == -EAGAIN || err == -EINTR || err == -ERESTARTSYS)
 		goto retry;
 
+	i915_gem_vm_bind_unlock(ce->vm);
+
 	if (err) {
 		struct i915_request *rq;
 
diff --git a/drivers/gpu/drm/i915/gem/i915_gem_object.c b/drivers/gpu/drm/i915/gem/i915_gem_object.c
index d2acab5dc7ea..bd474d349c3a 100644
--- a/drivers/gpu/drm/i915/gem/i915_gem_object.c
+++ b/drivers/gpu/drm/i915/gem/i915_gem_object.c
@@ -878,6 +878,8 @@ void __i915_gem_free_object(struct drm_i915_gem_object *obj)
 	if (obj->mm.n_placements > 1)
 		kfree(obj->mm.placements);
 
+	if (obj->shares_resv_from)
+		i915_vm_resv_put(obj->shares_resv_from);
 	i915_resv_put(obj->shares_resv);
 }
 
diff --git a/drivers/gpu/drm/i915/gem/i915_gem_object.h b/drivers/gpu/drm/i915/gem/i915_gem_object.h
index eb20863b62ba..f4f12312b630 100644
--- a/drivers/gpu/drm/i915/gem/i915_gem_object.h
+++ b/drivers/gpu/drm/i915/gem/i915_gem_object.h
@@ -214,6 +214,32 @@ static inline void assert_object_held_shared(struct drm_i915_gem_object *obj)
 		assert_object_held(obj);
 }
 
+static inline int
+i915_gem_object_lock_to_evict(struct drm_i915_gem_object *obj,
+			      struct i915_gem_ww_ctx *ww)
+{
+	int ret;
+
+	if (ww->intr)
+		ret = dma_resv_lock_interruptible(obj->base.resv, &ww->ctx);
+	else
+		ret = dma_resv_lock(obj->base.resv, &ww->ctx);
+
+	if (!ret) {
+		list_add_tail(&obj->obj_link, &ww->eviction_list);
+		i915_gem_object_get(obj);
+		obj->evict_locked = true;
+	}
+
+	GEM_WARN_ON(ret == -EALREADY);
+	if (ret == -EDEADLK) {
+		ww->contended_evict = true;
+		ww->contended = i915_gem_object_get(obj);
+	}
+
+	return ret;
+}
+
 static inline int __i915_gem_object_lock(struct drm_i915_gem_object *obj,
 					 struct i915_gem_ww_ctx *ww,
 					 bool intr)
diff --git a/drivers/gpu/drm/i915/gem/i915_gem_object_types.h b/drivers/gpu/drm/i915/gem/i915_gem_object_types.h
index bc48931253ba..532ad12b3bbf 100644
--- a/drivers/gpu/drm/i915/gem/i915_gem_object_types.h
+++ b/drivers/gpu/drm/i915/gem/i915_gem_object_types.h
@@ -277,6 +277,10 @@ struct drm_i915_gem_object {
 	 * when i915_gem_ww_ctx_backoff() or i915_gem_ww_ctx_fini() are called.
 	 */
 	struct list_head obj_link;
+	/**
+	 * @shared_resv_from: The object shares the resv from this vm.
+	 */
+	struct i915_address_space *shares_resv_from;
 	struct i915_resv *shares_resv;
 
 	union {
diff --git a/drivers/gpu/drm/i915/gem/i915_gem_pages.c b/drivers/gpu/drm/i915/gem/i915_gem_pages.c
index b195a52bd3c1..1154c9e66f4c 100644
--- a/drivers/gpu/drm/i915/gem/i915_gem_pages.c
+++ b/drivers/gpu/drm/i915/gem/i915_gem_pages.c
@@ -364,10 +364,6 @@ int __i915_gem_object_put_pages(struct drm_i915_gem_object *obj)
 	if (obj->mm.madv != I915_MADV_WILLNEED)
 		i915_gem_object_truncate(obj);
 
-	/* delete stale fences */
-	if (kref_read(&obj->base.refcount))
-		dma_resv_add_excl_fence(obj->base.resv, NULL);
-
 	return 0;
 }
 
diff --git a/drivers/gpu/drm/i915/gem/i915_gem_shrinker.c b/drivers/gpu/drm/i915/gem/i915_gem_shrinker.c
index 62270904cf6c..ee9fbb72d793 100644
--- a/drivers/gpu/drm/i915/gem/i915_gem_shrinker.c
+++ b/drivers/gpu/drm/i915/gem/i915_gem_shrinker.c
@@ -39,8 +39,8 @@ static bool can_release_pages(struct drm_i915_gem_object *obj)
 }
 
 static int drop_pages(struct drm_i915_gem_object *obj,
-		      struct i915_gem_ww_ctx *ww,
-		      unsigned long shrink, bool trylock_vm)
+		      struct i915_gem_ww_ctx *ww, unsigned long shrink,
+		      bool trylock_vm)
 {
 	unsigned long flags;
 
@@ -182,11 +182,6 @@ i915_gem_shrink(struct i915_gem_ww_ctx *ww,
 		       (obj = list_first_entry_or_null(phase->list,
 						       typeof(*obj),
 						       mm.link))) {
-			if (signal_pending(current)) {
-				err = -EINTR;
-				break;
-			}
-
 			list_move_tail(&obj->mm.link, &still_in_list);
 
 			/* only segment BOs should be in i915->mm.shrink.list */
@@ -212,36 +207,47 @@ i915_gem_shrink(struct i915_gem_ww_ctx *ww,
 				continue;
 
 			spin_unlock_irqrestore(&i915->mm.obj_lock, flags);
-
-			err = __i915_gem_object_lock_to_evict(obj, ww);
-			if (err)
-				goto skip;
+			if (ww) {
+				err = i915_gem_object_lock_to_evict(obj, ww);
+				if (err && err != -EALREADY)
+					goto skip;
+			} else {
+				if (!i915_gem_object_trylock(obj))
+					goto skip;
+			}
 
 			err = drop_pages(obj, ww, shrink, trylock_vm);
-			if (err == 0)
-				err = __i915_gem_object_put_pages(obj);
-			if (err == 0) {
-				try_to_writeback(obj, shrink);
-				count += obj->base.size >> PAGE_SHIFT;
+			if (!err) {
+				if (!__i915_gem_object_put_pages(obj)) {
+					try_to_writeback(obj, shrink);
+					count += obj->base.size >> PAGE_SHIFT;
+				}
 			}
 
-			i915_gem_object_unlock(obj);
-			scanned += obj->base.size >> PAGE_SHIFT;
+			/* If error is not EDEADLK or EINTR, skip object */
+			if (err != -EDEADLK && err != -EINTR)
+				err = 0;
 
+			if (!ww)
+				i915_gem_object_unlock(obj);
+
+			dma_resv_prune(obj->base.resv);
+
+			scanned += obj->base.size >> PAGE_SHIFT;
 skip:
 			i915_gem_object_put(obj);
-			spin_lock_irqsave(&i915->mm.obj_lock, flags);
 
-			if (err == -EDEADLK)
+			spin_lock_irqsave(&i915->mm.obj_lock, flags);
+			if (err)
 				break;
-
-			err = 0;
 		}
 		list_splice_tail(&still_in_list, phase->list);
 		spin_unlock_irqrestore(&i915->mm.obj_lock, flags);
 		if (err)
 			break;
 	}
+	if (ww)
+		i915_gem_ww_ctx_unlock_evictions(ww);
 
 	if (shrink & I915_SHRINK_BOUND)
 		intel_runtime_pm_put(&i915->runtime_pm, wakeref);
diff --git a/drivers/gpu/drm/i915/gem/i915_gem_vm_bind.h b/drivers/gpu/drm/i915/gem/i915_gem_vm_bind.h
index 3fae0fad94b2..73a87e5e08f3 100644
--- a/drivers/gpu/drm/i915/gem/i915_gem_vm_bind.h
+++ b/drivers/gpu/drm/i915/gem/i915_gem_vm_bind.h
@@ -26,6 +26,30 @@ static inline void i915_gem_vm_bind_unlock(struct i915_address_space *vm)
 	mutex_unlock(&vm->vm_bind_lock);
 }
 
+#define assert_vm_priv_held(vm)   assert_object_held((vm)->root_obj)
+
+static inline int i915_gem_vm_priv_lock(struct i915_address_space *vm,
+					struct i915_gem_ww_ctx *ww)
+{
+	return i915_gem_object_lock(vm->root_obj, ww);
+}
+
+static inline int i915_gem_vm_priv_trylock(struct i915_address_space *vm)
+{
+	return i915_gem_object_trylock(vm->root_obj) ? 0 : -EBUSY;
+}
+
+static inline int i915_gem_vm_priv_lock_to_evict(struct i915_address_space *vm,
+						 struct i915_gem_ww_ctx *ww)
+{
+	return i915_gem_object_lock_to_evict(vm->root_obj, ww);
+}
+
+static inline void i915_gem_vm_priv_unlock(struct i915_address_space *vm)
+{
+	i915_gem_object_unlock(vm->root_obj);
+}
+
 struct i915_vma *
 i915_gem_vm_bind_lookup_vma(struct i915_address_space *vm, u64 va);
 int i915_gem_vm_bind_obj(struct i915_address_space *vm,
diff --git a/drivers/gpu/drm/i915/gem/i915_gem_vm_bind_object.c b/drivers/gpu/drm/i915/gem/i915_gem_vm_bind_object.c
index 5ca137f717e9..28f0045229a0 100644
--- a/drivers/gpu/drm/i915/gem/i915_gem_vm_bind_object.c
+++ b/drivers/gpu/drm/i915/gem/i915_gem_vm_bind_object.c
@@ -9,7 +9,6 @@
 #include "gt/gen8_engine_cs.h"
 
 #include "i915_drv.h"
-#include "i915_gem_context.h"
 #include "i915_gem_gtt.h"
 #include "i915_gem_vm_bind.h"
 #include "i915_sw_fence_work.h"
@@ -29,8 +28,8 @@ struct vm_bind_user_ext {
 	struct drm_i915_gem_object *obj;
 };
 
-#define START(x) ((x)->node.start)
-#define LAST(x) ((x)->node.start + (x)->node.size - 1)
+#define START(node) ((node)->start)
+#define LAST(node) ((node)->last)
 
 INTERVAL_TREE_DEFINE(struct i915_vma, rb, u64, __subtree_last,
 		     START, LAST, static inline, i915_vm_bind_it)
@@ -307,8 +306,12 @@ static void i915_gem_vm_bind_remove(struct i915_vma *vma)
 		list_del_init(&vma->vm_capture_link);
 	spin_unlock(&vma->vm->vm_capture_lock);
 
-	list_del_init(&vma->vm_bind_link);
+	spin_lock(&vma->vm->vm_rebind_lock);
+	list_del(&vma->vm_rebind_link);
+	spin_unlock(&vma->vm->vm_rebind_lock);
 
+	list_del_init(&vma->vm_bind_link);
+	list_del_init(&vma->non_priv_vm_bind_link);
 	i915_vm_bind_it_remove(vma, &vma->vm->va);
 }
 
@@ -317,7 +320,6 @@ void i915_gem_vm_unbind_all(struct i915_address_space *vm)
 	struct i915_vma *vma, *vn;
 
 	i915_gem_vm_bind_lock(vm);
-	i915_gem_object_lock(vm->root_obj, NULL);
 	list_for_each_entry_safe(vma, vn, &vm->vm_bind_list, vm_bind_link) {
 		i915_gem_vm_bind_remove(vma);
 		i915_gem_vm_bind_release(vma);
@@ -326,7 +328,6 @@ void i915_gem_vm_unbind_all(struct i915_address_space *vm)
 		i915_gem_vm_bind_remove(vma);
 		i915_gem_vm_bind_release(vma);
 	}
-	i915_gem_object_unlock(vm->root_obj);
 	i915_gem_vm_bind_unlock(vm);
 }
 
@@ -340,12 +341,8 @@ struct unbind_work {
 static int unbind(struct dma_fence_work *work)
 {
 	struct unbind_work *w = container_of(work, typeof(*w), base);
-	struct i915_address_space *vm = w->vma->vm;
 
-	i915_gem_object_lock(vm->root_obj, NULL);
 	i915_gem_vm_bind_unpublish(w->vma);
-	i915_gem_object_unlock(vm->root_obj);
-
 	return 0;
 }
 
@@ -354,7 +351,7 @@ static void release(struct dma_fence_work *work)
 	struct unbind_work *w = container_of(work, typeof(*w), base);
 
 	i915_debugger_vma_purge(w->vma->vm->client, w->vma);
-	__i915_vma_put(w->vma);
+	i915_gem_vm_bind_release(w->vma);
 }
 
 static const struct dma_fence_work_ops unbind_ops = {
@@ -374,7 +371,7 @@ static struct dma_fence_work *queue_unbind(struct i915_vma *vma,
 		return NULL;
 
 	dma_fence_work_init(&w->base, &unbind_ops);
-	w->vma = __i915_vma_get(vma);
+	w->vma = vma;
 	INIT_LIST_HEAD(&w->unbind_link);
 	list_add_tail(&w->unbind_link, unbind_head);
 
@@ -408,6 +405,11 @@ static int verify_adjacent_segments(struct i915_vma *vma, u64 length)
 	 * release active references on error.
 	 */
 	while (vma && (vma_size < length)) {
+		if (i915_vma_is_pinned(vma) || atomic_read(&vma->open_count)) {
+			ret = -EAGAIN;
+			goto out_del;
+		}
+
 		ret = i915_active_acquire(&vma->active);
 		if (ret) {
 			vma_end = vma;
@@ -434,56 +436,12 @@ static int verify_adjacent_segments(struct i915_vma *vma, u64 length)
 	return ret;
 }
 
-static void
-lut_remove(struct drm_i915_gem_object *obj, struct i915_address_space *vm)
-{
-	struct i915_lut_handle bookmark = {};
-	struct i915_lut_handle *lut, *ln;
-	LIST_HEAD(close);
-
-	spin_lock(&obj->lut_lock);
-	list_for_each_entry_safe(lut, ln, &obj->lut_list, obj_link) {
-		struct i915_gem_context *ctx = lut->ctx;
-
-		if (ctx && ctx->client == vm->client) {
-			i915_gem_context_get(ctx);
-			list_move(&lut->obj_link, &close);
-		}
-
-		/* Break long locks, and carefully continue on from this spot */
-		if (&ln->obj_link != &obj->lut_list) {
-			list_add_tail(&bookmark.obj_link, &ln->obj_link);
-			if (cond_resched_lock(&obj->lut_lock))
-				list_safe_reset_next(&bookmark, ln, obj_link);
-			__list_del_entry(&bookmark.obj_link);
-		}
-	}
-	spin_unlock(&obj->lut_lock);
-
-	list_for_each_entry_safe(lut, ln, &close, obj_link) {
-		struct i915_gem_context *ctx = lut->ctx;
-		struct i915_vma *vma;
-
-		mutex_lock(&ctx->lut_mutex);
-		vma = radix_tree_delete(&ctx->handles_vma, lut->handle);
-		if (vma) {
-			GEM_BUG_ON(vma->obj != obj);
-			GEM_BUG_ON(!atomic_read(&vma->open_count));
-			i915_vma_close(vma);
-		}
-		mutex_unlock(&ctx->lut_mutex);
-
-		i915_gem_context_put(lut->ctx);
-		i915_lut_handle_free(lut);
-		i915_gem_object_put(obj);
-	}
-}
-
 int i915_gem_vm_unbind_obj(struct i915_address_space *vm,
 			   struct prelim_drm_i915_gem_vm_bind *va)
 {
 	struct i915_vma *vma, *vma_head, *vma_next;
 	struct unbind_work *uwn, *uw = NULL;
+	struct dma_fence_work *work = NULL;
 	LIST_HEAD(unbind_head);
 	int ret;
 
@@ -513,12 +471,14 @@ int i915_gem_vm_unbind_obj(struct i915_address_space *vm,
 			goto out_unlock;
 		}
 
-		if (test_bit(I915_VMA_HAS_LUT_BIT, __i915_vma_flags(vma)))
-			lut_remove(vma->obj, vm);
+		if (i915_vma_is_pinned(vma) || atomic_read(&vma->open_count)) {
+			ret = -EAGAIN;
+			goto out_unlock;
+		}
 
-		mutex_lock(&vm->mutex);
 		ret = i915_active_acquire(&vma->active);
-		mutex_unlock(&vm->mutex);
+		if (ret)
+			goto out_unlock;
 	} else {
 		/*
 		 * For support of segmented BOs, we make EAGAIN checks for each
@@ -526,44 +486,39 @@ int i915_gem_vm_unbind_obj(struct i915_address_space *vm,
 		 * set of VMAs. This will do active_acquire() for each segment;
 		 * but on error, these are released before returning.
 		 */
-		mutex_lock(&vm->mutex);
 		ret = verify_adjacent_segments(vma, va->length);
-		mutex_unlock(&vm->mutex);
+		if (ret)
+			goto out_unlock;
 	}
-	if (ret)
-		goto out_unlock;
 
 	/*
 	 * As this uses dma_fence_work, we use multiple workers (one per VMA)
 	 * as each worker advances the vma->active timeline.
 	 */
 	for (vma = vma_head; vma; vma = vma->adjacent_next) {
-		if (!queue_unbind(vma, &unbind_head)) {
+		work = queue_unbind(vma, &unbind_head);
+		if (!work) {
 			ret = -ENOMEM;
+			list_for_each_entry_safe(uw, uwn, &unbind_head, unbind_link)
+				dma_fence_put(&uw->base.dma);
 			break;
 		}
 	}
 
-	i915_gem_object_lock(vm->root_obj, NULL);
 	for (vma = vma_head; vma; vma = vma_next) {
 		i915_active_release(&vma->active);
 		vma_next = vma->adjacent_next;
 		if (!ret) {
 			vma->adjacent_next = NULL;
 			i915_gem_vm_bind_remove(vma);
-			i915_gem_vm_bind_release(vma);
 		}
 	}
-	i915_gem_object_unlock(vm->root_obj);
 
 out_unlock:
 	i915_gem_vm_bind_unlock(vm);
 
 	list_for_each_entry_safe(uw, uwn, &unbind_head, unbind_link) {
-		struct dma_fence_work *work = &uw->base;
-
-		i915_sw_fence_set_error_once(&work->chain, ret);
-
+		work = &uw->base;
 		dma_fence_get(&work->dma);
 		dma_fence_work_commit_imm(work);
 		if (!vm->i915->params.async_vm_unbind)
@@ -588,8 +543,8 @@ static struct i915_vma *vm_create_vma(struct i915_address_space *vm,
 	if (IS_ERR(vma))
 		return vma;
 
-	vma->node.start = start;
-	vma->node.size = size;
+	vma->start = start;
+	vma->last = start + size - 1;
 	__set_bit(I915_VMA_PERSISTENT_BIT, __i915_vma_flags(vma));
 
 	return __i915_vma_get(vma);
@@ -682,32 +637,43 @@ static int vm_bind_get_vmas(struct i915_address_space *vm,
 	return err;
 }
 
-static int vma_bind_insert(struct i915_vma *vma, u64 pin_flags)
+static int vma_bind_insert(struct i915_vma *vma, struct i915_gem_ww_ctx *ww,
+			   u64 pin_flags)
 {
 	struct i915_address_space *vm = vma->vm;
-	struct i915_gem_ww_ctx ww;
 	int ret = 0;
 
-	for_i915_gem_ww(&ww, ret, true) {
-		ret = i915_gem_object_lock(vm->root_obj, &ww);
+retry:
+	if (pin_flags) {
+		/* Always take vm_priv lock here (just like execbuff path) even
+		 * for shared BOs, this will prevent the eviction/shrinker logic
+		 * from evicint private BOs of the VM.
+		 */
+		ret = i915_gem_vm_priv_lock(vm, ww);
 		if (ret)
-			continue;
+			goto out_ww;
 
-		if (pin_flags) {
-			ret = i915_gem_object_lock(vma->obj, &ww);
-			if (ret)
-				continue;
+		ret = i915_gem_object_lock(vma->obj, ww);
+		if (ret)
+			goto out_ww;
 
-			ret = i915_vma_pin_ww(vma, &ww, 0, 0,
-					      vma->node.start | pin_flags);
-			if (ret)
-				continue;
+		ret = i915_vma_pin_ww(vma, ww, 0, 0, pin_flags);
+		if (ret)
+			goto out_ww;
 
-			__i915_vma_unpin(vma);
-		}
+		__i915_vma_unpin(vma);
+	}
 
-		list_move_tail(&vma->vm_bind_link, &vm->vm_bind_list);
-		i915_vm_bind_it_insert(vma, &vm->va);
+	list_move_tail(&vma->vm_bind_link, &vm->vm_bind_list);
+	i915_vm_bind_it_insert(vma, &vm->va);
+	if (!vma->obj->vm)
+		list_add_tail(&vma->non_priv_vm_bind_link,
+			      &vm->non_priv_vm_bind_list);
+out_ww:
+	if (ret == -EDEADLK) {
+		ret = i915_gem_ww_ctx_backoff(ww);
+		if (!ret)
+			goto retry;
 	}
 
 	return ret;
@@ -725,6 +691,7 @@ int i915_gem_vm_bind_obj(struct i915_address_space *vm,
 	struct i915_sw_fence *bind_fence = NULL;
 	struct drm_i915_gem_object *obj;
 	struct i915_vma *vma, *vma_next;
+	struct i915_gem_ww_ctx ww;
 	LIST_HEAD(vma_head);
 	u64 pin_flags = 0;
 	int ret;
@@ -773,7 +740,7 @@ int i915_gem_vm_bind_obj(struct i915_address_space *vm,
 	}
 
 	if (va->flags & PRELIM_I915_GEM_VM_BIND_IMMEDIATE) {
-		pin_flags = PIN_OFFSET_FIXED | PIN_USER;
+		pin_flags = va->start | PIN_OFFSET_FIXED | PIN_USER;
 		if (va->flags & PRELIM_I915_GEM_VM_BIND_MAKE_RESIDENT)
 			pin_flags |= PIN_RESIDENT;
 
@@ -814,11 +781,12 @@ int i915_gem_vm_bind_obj(struct i915_address_space *vm,
 	/* The metadata is in VMA so debugger can prepare the event properly */
 	i915_debugger_vma_prepare(vm->client, vma, PRELIM_DRM_I915_DEBUG_EVENT_CREATE);
 
+	i915_gem_ww_ctx_init(&ww, true);
 	list_for_each_entry_safe(vma, vma_next, &vma_head, vm_bind_link) {
 		/* Hold object reference until vm_unbind */
 		i915_gem_object_get(vma->obj);
 
-		ret = vma_bind_insert(vma, pin_flags);
+		ret = vma_bind_insert(vma, &ww, pin_flags);
 		if (ret)
 			break;
 
@@ -832,6 +800,10 @@ int i915_gem_vm_bind_obj(struct i915_address_space *vm,
 			spin_unlock(&vm->vm_capture_lock);
 		}
 
+		/* increment va->start (embedded in pin_flags) */
+		if (pin_flags)
+			pin_flags += vma->size;
+
 		/* adjacency list is for use in unbind and capture_vma */
 		if (vma_prev)
 			vma_prev->adjacent_next = vma;
@@ -839,12 +811,10 @@ int i915_gem_vm_bind_obj(struct i915_address_space *vm,
 			adjacency_start = vma;
 		vma_prev = vma;
 	}
-
+	i915_gem_ww_ctx_fini(&ww);
 	set_bit(I915_VM_HAS_PERSISTENT_BINDS, &vm->flags);
 
 	if (ret) {
-		i915_gem_object_lock(vm->root_obj, NULL);
-
 		/* cleanup VMAs where vma_bind_insert() succeeded */
 		for (vma = adjacency_start; vma; vma = vma_next) {
 			vma_next = vma->adjacent_next;
@@ -858,8 +828,6 @@ int i915_gem_vm_bind_obj(struct i915_address_space *vm,
 			i915_gem_vm_bind_unpublish(vma);
 			i915_gem_vm_bind_release(vma);
 		}
-
-		i915_gem_object_unlock(vm->root_obj);
 	}
 
 unlock_vm:
diff --git a/drivers/gpu/drm/i915/gem/selftests/i915_gem_execbuffer.c b/drivers/gpu/drm/i915/gem/selftests/i915_gem_execbuffer.c
index 8e6d6f722c8c..290682d4af3c 100644
--- a/drivers/gpu/drm/i915/gem/selftests/i915_gem_execbuffer.c
+++ b/drivers/gpu/drm/i915/gem/selftests/i915_gem_execbuffer.c
@@ -69,15 +69,20 @@ static int suspend_request(struct intel_gt *gt, struct intel_context *ce,
 	engine = READ_ONCE(rq->engine);
 	/* We have our request executing, now suspend it. */
 
-	i915_vm_lock_objects(vm, NULL);
+	i915_gem_vm_bind_lock(vm);
 
 	fs_reclaim_acquire(GFP_KERNEL);
 	timeout = dma_fence_wait_timeout(&ce->sfence->base.dma, true, delay);
 	fs_reclaim_release(GFP_KERNEL);
-	if (timeout <= 0) {
+	if (timeout == 0) {
 		pr_err("%s: Suspend running request timeout.\n", engine->name);
-		i915_gem_object_unlock(vm->root_obj);
-		err = timeout ?: -ETIME;
+		err = -ETIME;
+		i915_gem_vm_bind_unlock(vm);
+		goto out;
+	}
+	if (timeout < 0) {
+		i915_gem_vm_bind_unlock(vm);
+		err = timeout;
 		goto out;
 	}
 
@@ -93,13 +98,13 @@ static int suspend_request(struct intel_gt *gt, struct intel_context *ce,
 	if (i915_request_completed(rq)) {
 		pr_err("%s: suspended request completed!\n",
 		       engine->name);
-		i915_gem_object_unlock(vm->root_obj);
+		i915_gem_vm_bind_unlock(vm);
 		err = -EIO;
 		goto out;
 	}
 
 	/* But completes on resume */
-	i915_gem_object_unlock(vm->root_obj);
+	i915_gem_vm_bind_unlock(vm);
 	if (i915_request_wait(rq, 0, delay) < 0) {
 		pr_err("%s: resumed request did not complete!\n",
 		       engine->name);
diff --git a/drivers/gpu/drm/i915/gt/intel_ggtt.c b/drivers/gpu/drm/i915/gt/intel_ggtt.c
index d2515f72443f..e2c8d1f400e9 100644
--- a/drivers/gpu/drm/i915/gt/intel_ggtt.c
+++ b/drivers/gpu/drm/i915/gt/intel_ggtt.c
@@ -70,6 +70,11 @@ static void i915_ggtt_color_adjust(const struct drm_mm_node *node,
 static int ggtt_init_hw(struct i915_ggtt *ggtt)
 {
 	struct drm_i915_private *i915 = ggtt->vm.i915;
+	int err;
+
+	err = i915_address_space_init(&ggtt->vm, VM_CLASS_GGTT);
+	if (err)
+		return err;
 
 	ggtt->vm.is_ggtt = true;
 
@@ -1211,6 +1216,9 @@ void i915_ggtt_driver_late_release(struct drm_i915_private *i915)
 		if (gt->type == GT_MEDIA)
 			continue;
 
+		GEM_WARN_ON(kref_read(&ggtt->vm.resv_ref) != 1);
+		dma_resv_fini(&ggtt->vm._resv);
+
 		kfree(ggtt);
 	}
 }
@@ -1270,10 +1278,6 @@ static int ggtt_probe_common(struct i915_ggtt *ggtt, u64 size)
 	phys_addr_t phys_addr;
 	int ret;
 
-	ret = i915_address_space_init(&ggtt->vm, VM_CLASS_GGTT);
-	if (ret)
-		return ret;
-
 	phys_addr = ggtt->vm.gt->phys_addr + gen6_gttadr_offset(i915);
 
 	/*
@@ -1292,6 +1296,7 @@ static int ggtt_probe_common(struct i915_ggtt *ggtt, u64 size)
 		return -ENOMEM;
 	}
 
+	kref_init(&ggtt->vm.resv_ref);
 	ret = i915_vm_setup_scratch0(&ggtt->vm);
 	if (ret) {
 		drm_err(&i915->drm, "Scratch setup failed\n");
@@ -1618,6 +1623,7 @@ static int ggtt_probe_hw(struct i915_ggtt *ggtt, struct intel_gt *gt)
 	ggtt->vm.gt = gt;
 	ggtt->vm.i915 = i915;
 	ggtt->vm.dma = i915->drm.dev;
+	dma_resv_init(&ggtt->vm._resv);
 
 	if (IS_SRIOV_VF(i915))
 		ret = gen12vf_ggtt_probe(ggtt);
@@ -1627,8 +1633,11 @@ static int ggtt_probe_hw(struct i915_ggtt *ggtt, struct intel_gt *gt)
 		ret = gen6_gmch_probe(ggtt);
 	else
 		ret = intel_ggtt_gmch_probe(ggtt);
-	if (ret)
+
+	if (ret) {
+		dma_resv_fini(&ggtt->vm._resv);
 		return ret;
+	}
 
 	if ((ggtt->vm.total - 1) >> 32) {
 		drm_err(&i915->drm,
diff --git a/drivers/gpu/drm/i915/gt/intel_gtt.c b/drivers/gpu/drm/i915/gt/intel_gtt.c
index 4b2264d9daf6..ca3fa054ad67 100644
--- a/drivers/gpu/drm/i915/gt/intel_gtt.c
+++ b/drivers/gpu/drm/i915/gt/intel_gtt.c
@@ -46,8 +46,10 @@ struct drm_i915_gem_object *alloc_pt_lmem(struct i915_address_space *vm, int sz)
 	 * object underneath, with the idea that one object_lock() will lock
 	 * them all at once.
 	 */
-	if (!IS_ERR(obj))
-		i915_gem_object_share_resv(vm->root_obj, obj);
+	if (!IS_ERR(obj)) {
+		obj->base.resv = i915_vm_resv_get(vm);
+		obj->shares_resv_from = vm;
+	}
 
 	return obj;
 }
@@ -67,7 +69,8 @@ struct drm_i915_gem_object *alloc_pt_dma(struct i915_address_space *vm, int sz)
 	 */
 	if (!IS_ERR(obj)) {
 		obj->flags |= I915_BO_ALLOC_CONTIGUOUS;
-		i915_gem_object_share_resv(vm->root_obj, obj);
+		obj->base.resv = i915_vm_resv_get(vm);
+		obj->shares_resv_from = vm;
 	}
 
 	return obj;
@@ -125,10 +128,18 @@ static void __i915_vm_close(struct i915_address_space *vm)
 	}
 }
 
-int i915_vm_lock_objects(const struct i915_address_space *vm,
+/* lock the vm into the current ww, if we lock one, we lock all */
+int i915_vm_lock_objects(struct i915_address_space *vm,
 			 struct i915_gem_ww_ctx *ww)
 {
-	return i915_gem_object_lock(vm->root_obj, ww);
+	if (vm->scratch[0] && (vm->scratch[0]->base.resv == &vm->_resv)) {
+		return i915_gem_object_lock(vm->scratch[0], ww);
+	} else {
+		struct i915_ppgtt *ppgtt = i915_vm_to_ppgtt(vm);
+
+		/* We borrowed the scratch page from ggtt, take the top level object */
+		return i915_gem_object_lock(ppgtt->pd->pt.base, ww);
+	}
 }
 
 void i915_address_space_fini(struct i915_address_space *vm)
@@ -153,6 +164,22 @@ void i915_address_space_fini(struct i915_address_space *vm)
 	iput(vm->inode);
 }
 
+/**
+ * i915_vm_resv_release - Final struct i915_address_space destructor
+ * @kref: Pointer to the &i915_address_space.resv_ref member.
+ *
+ * This function is called when the last lock sharer no longer shares the
+ * &i915_address_space._resv lock.
+ */
+void i915_vm_resv_release(struct kref *kref)
+{
+	struct i915_address_space *vm =
+		container_of(kref, typeof(*vm), resv_ref);
+
+	dma_resv_fini(&vm->_resv);
+	kfree(vm);
+}
+
 static void __i915_vm_release(struct work_struct *work)
 {
 	struct i915_address_space *vm =
@@ -162,6 +189,8 @@ static void __i915_vm_release(struct work_struct *work)
 	i915_svm_unbind_mm(vm);
 	vm->cleanup(vm);
 	i915_address_space_fini(vm);
+
+	i915_vm_resv_put(vm);
 }
 
 void i915_vm_release(struct kref *kref)
@@ -216,6 +245,13 @@ int i915_address_space_init(struct i915_address_space *vm, int subclass)
 
 	kref_init(&vm->ref);
 
+	/*
+	 * Special case for GGTT that has already done an early
+	 * kref_init here.
+	 */
+	if (!kref_read(&vm->resv_ref))
+		kref_init(&vm->resv_ref);
+
 	INIT_RCU_WORK(&vm->rcu, __i915_vm_release);
 	atomic_set(&vm->open, 1);
 	INIT_WORK(&vm->close_work, i915_vm_close_work);
@@ -244,6 +280,7 @@ int i915_address_space_init(struct i915_address_space *vm, int subclass)
 		might_alloc(GFP_KERNEL);
 		mutex_release(&vm->mutex.dep_map, _THIS_IP_);
 	}
+	dma_resv_init(&vm->_resv);
 
 	vm->inode = alloc_anon_inode(vm->i915->drm.anon_inode->i_sb);
 	if (IS_ERR(vm->inode))
@@ -278,15 +315,16 @@ int i915_address_space_init(struct i915_address_space *vm, int subclass)
 	INIT_LIST_HEAD(&vm->vm_bind_list);
 	INIT_LIST_HEAD(&vm->vm_bound_list);
 	mutex_init(&vm->vm_bind_lock);
-
+	INIT_LIST_HEAD(&vm->non_priv_vm_bind_list);
 	vm->root_obj = i915_gem_object_create_internal(vm->i915, PAGE_SIZE);
-	if (IS_ERR_OR_NULL(vm->root_obj))
-		return -ENOMEM;
+	GEM_BUG_ON(IS_ERR(vm->root_obj));
 
 	spin_lock_init(&vm->priv_obj_lock);
 	INIT_LIST_HEAD(&vm->priv_obj_list);
 	INIT_LIST_HEAD(&vm->vm_capture_list);
 	spin_lock_init(&vm->vm_capture_lock);
+	INIT_LIST_HEAD(&vm->vm_rebind_list);
+	spin_lock_init(&vm->vm_rebind_lock);
 	INIT_ACTIVE_FENCE(&vm->user_fence);
 
 	vm->has_scratch = true;
diff --git a/drivers/gpu/drm/i915/gt/intel_gtt.h b/drivers/gpu/drm/i915/gt/intel_gtt.h
index f9d7f4625b84..5012a8528199 100644
--- a/drivers/gpu/drm/i915/gt/intel_gtt.h
+++ b/drivers/gpu/drm/i915/gt/intel_gtt.h
@@ -334,6 +334,8 @@ struct i915_address_space {
 
 	struct mutex mutex; /* protects vma and our lists */
 
+	struct kref resv_ref; /* kref to keep the reservation lock alive. */
+	struct dma_resv _resv; /* reservation lock for all pd objects, and buffer pool */
 #define VM_CLASS_GGTT 0
 #define VM_CLASS_PPGTT 1
 #define VM_CLASS_DPT 2
@@ -353,9 +355,12 @@ struct i915_address_space {
 	struct list_head vm_bind_list;
 	struct list_head vm_bound_list;
 	struct list_head vm_capture_list;
+	struct list_head vm_rebind_list;
 	spinlock_t vm_capture_lock;  /* Protects vm_capture_list */
+	spinlock_t vm_rebind_lock;   /* Protects vm_rebind_list */
 	/* va tree of persistent vmas */
 	struct rb_root_cached va;
+	struct list_head non_priv_vm_bind_list;
 	struct drm_i915_gem_object *root_obj;
 
 	spinlock_t priv_obj_lock;
@@ -530,9 +535,8 @@ struct i915_ppgtt {
 
 bool intel_vm_no_concurrent_access_wa(struct drm_i915_private *i915);
 
-/* lock the vm into the current ww, if we lock one, we lock all */
-int i915_vm_lock_objects(const struct i915_address_space *vm,
-			 struct i915_gem_ww_ctx *ww);
+int __must_check
+i915_vm_lock_objects(struct i915_address_space *vm, struct i915_gem_ww_ctx *ww);
 
 static inline unsigned int
 i915_vm_lvl(const struct i915_address_space * const vm)
@@ -593,6 +597,18 @@ i915_vm_get(struct i915_address_space *vm)
 	return vm;
 }
 
+/**
+ * i915_vm_resv_get - Obtain a reference on the vm's reservation lock
+ * @vm: The vm whose reservation lock we want to share.
+ *
+ * Return: A pointer to the vm's reservation lock.
+ */
+static inline struct dma_resv *i915_vm_resv_get(struct i915_address_space *vm)
+{
+	kref_get(&vm->resv_ref);
+	return &vm->_resv;
+}
+
 static inline struct i915_address_space *
 i915_vm_tryget(struct i915_address_space *vm)
 {
@@ -604,11 +620,22 @@ i915_vm_tryget(struct i915_address_space *vm)
 
 void i915_vm_release(struct kref *kref);
 
+void i915_vm_resv_release(struct kref *kref);
+
 static inline void i915_vm_put(struct i915_address_space *vm)
 {
 	kref_put(&vm->ref, i915_vm_release);
 }
 
+/**
+ * i915_vm_resv_put - Release a reference on the vm's reservation lock
+ * @resv: Pointer to a reservation lock obtained from i915_vm_resv_get()
+ */
+static inline void i915_vm_resv_put(struct i915_address_space *vm)
+{
+	kref_put(&vm->resv_ref, i915_vm_resv_release);
+}
+
 static inline struct i915_address_space *
 i915_vm_open(struct i915_address_space *vm)
 {
diff --git a/drivers/gpu/drm/i915/gt/intel_pagefault.c b/drivers/gpu/drm/i915/gt/intel_pagefault.c
index fb5127e3bbee..7d0cf936d295 100644
--- a/drivers/gpu/drm/i915/gt/intel_pagefault.c
+++ b/drivers/gpu/drm/i915/gt/intel_pagefault.c
@@ -694,24 +694,36 @@ const char *intel_acc_err2str(unsigned int err)
 static int acc_migrate_to_lmem(struct intel_gt *gt, struct i915_vma *vma)
 {
 	struct i915_gem_ww_ctx ww;
-	enum intel_region_id lmem_id;
 	int err = 0;
 
-	if (!i915_vma_is_bound(vma, PIN_RESIDENT))
-		return 0;
+	i915_gem_vm_bind_lock(vma->vm);
 
-	lmem_id = get_lmem_region_id(vma->obj, gt);
-	if (!lmem_id)
+	if (!i915_vma_is_bound(vma, PIN_RESIDENT)) {
+		i915_gem_vm_bind_unlock(vma->vm);
 		return 0;
+	}
 
-	for_i915_gem_ww(&ww, err, false) {
-		err = i915_gem_object_lock(vma->obj, &ww);
-		if (err)
-			continue;
+	i915_gem_ww_ctx_init(&ww, false);
 
-		err = migrate_to_lmem(vma->obj, gt, lmem_id, &ww);
+retry:
+	err = i915_gem_object_lock(vma->obj, &ww);
+	if (!err) {
+		enum intel_region_id lmem_id;
+
+		lmem_id = get_lmem_region_id(vma->obj, gt);
+		if (lmem_id)
+			err = migrate_to_lmem(vma->obj, gt, lmem_id, &ww);
 	}
 
+	if (err == -EDEADLK) {
+		err = i915_gem_ww_ctx_backoff(&ww);
+		if (!err)
+			goto retry;
+	}
+
+	i915_gem_ww_ctx_fini(&ww);
+	i915_gem_vm_bind_unlock(vma->vm);
+
 	return err;
 }
 
diff --git a/drivers/gpu/drm/i915/gt/intel_ppgtt.c b/drivers/gpu/drm/i915/gt/intel_ppgtt.c
index 84b31e4f79df..6f63c6777d4e 100644
--- a/drivers/gpu/drm/i915/gt/intel_ppgtt.c
+++ b/drivers/gpu/drm/i915/gt/intel_ppgtt.c
@@ -431,6 +431,7 @@ int ppgtt_init(struct i915_ppgtt *ppgtt, struct intel_gt *gt)
 	else
 		ppgtt->vm.top = 1;
 
+	dma_resv_init(&ppgtt->vm._resv);
 	err = i915_address_space_init(&ppgtt->vm, VM_CLASS_PPGTT);
 	if (err)
 		return err;
diff --git a/drivers/gpu/drm/i915/i915_debugger.c b/drivers/gpu/drm/i915/i915_debugger.c
index 8849fbaec81f..8ff1d733148a 100644
--- a/drivers/gpu/drm/i915/i915_debugger.c
+++ b/drivers/gpu/drm/i915/i915_debugger.c
@@ -1659,7 +1659,7 @@ static ssize_t access_page_in_vm(struct i915_address_space *vm,
 		if (ret)
 			continue;
 
-		vma_offset = vm_offset - vma->node.start;
+		vma_offset = vm_offset - vma->start;
 
 		len = min_t(ssize_t, len, PAGE_SIZE - offset_in_page(vma_offset));
 
diff --git a/drivers/gpu/drm/i915/i915_gem.c b/drivers/gpu/drm/i915/i915_gem.c
index 997d7f5f4c3a..0a46d9e45d28 100644
--- a/drivers/gpu/drm/i915/i915_gem.c
+++ b/drivers/gpu/drm/i915/i915_gem.c
@@ -49,6 +49,7 @@
 #include "gem/i915_gem_mman.h"
 #include "gem/i915_gem_pm.h"
 #include "gem/i915_gem_region.h"
+#include "gem/i915_gem_vm_bind.h"
 #include "gt/intel_engine_user.h"
 #include "gt/intel_gt.h"
 #include "gt/intel_gt_pm.h"
@@ -122,6 +123,38 @@ i915_gem_get_aperture_ioctl(struct drm_device *dev, void *data,
 	return 0;
 }
 
+static int i915_vma_unbind_persistent(struct i915_vma *vma,
+				      struct i915_gem_ww_ctx *ww,
+				      unsigned long flags)
+{
+	int ret;
+
+	/* VM already locked */
+	if (ww && ww == i915_gem_get_locking_ctx(vma->vm->root_obj)) {
+		/* locked for other than unbinding? */
+		if (!(flags & I915_GEM_OBJECT_UNBIND_ACTIVE) &&
+		    (!vma->vm->root_obj->evict_locked))
+			return -EBUSY;
+
+		/* locked for unbinding */
+		goto already_locked;
+	}
+
+	if (!ww)
+		ret = i915_gem_vm_priv_trylock(vma->vm);
+	else
+		ret = i915_gem_vm_priv_lock_to_evict(vma->vm, ww);
+	if (ret)
+		return ret;
+
+already_locked:
+	ret = i915_vma_unbind(vma);
+	if (!ww)
+		i915_gem_vm_priv_unlock(vma->vm);
+
+	return ret;
+}
+
 /*
  * For segmented BOs, this routine should be called for just the individual
  * segments and not the parent BO. As only the individual segments have
@@ -136,7 +169,8 @@ int i915_gem_object_unbind(struct drm_i915_gem_object *obj,
 			   struct i915_gem_ww_ctx *ww,
 			   unsigned long flags)
 {
-	struct intel_runtime_pm *rpm = &to_i915(obj->base.dev)->runtime_pm;
+	struct drm_i915_private *i915 = to_i915(obj->base.dev);
+	struct intel_runtime_pm *rpm = &i915->runtime_pm;
 	intel_wakeref_t wakeref = 0;
 	LIST_HEAD(still_in_list);
 	struct i915_vma *vma;
@@ -145,6 +179,15 @@ int i915_gem_object_unbind(struct drm_i915_gem_object *obj,
 	if (list_empty(&obj->vma.list))
 		return 0;
 
+	/*
+	 * As some machines use ACPI to handle runtime-resume callbacks, and
+	 * ACPI is quite kmalloc happy, we cannot resume beneath the vm->mutex
+	 * as they are required by the shrinker. Ergo, we wake the device up
+	 * first just in case.
+	 */
+	if (!(flags & I915_GEM_OBJECT_UNBIND_TEST))
+		wakeref = intel_runtime_pm_get(rpm);
+
 try_again:
 	ret = 0;
 	spin_lock(&obj->vma.lock);
@@ -152,7 +195,6 @@ int i915_gem_object_unbind(struct drm_i915_gem_object *obj,
 						       struct i915_vma,
 						       obj_link))) {
 		struct i915_address_space *vm = vma->vm;
-		struct drm_i915_gem_object *unlock = NULL;
 
 		list_move_tail(&vma->obj_link, &still_in_list);
 		if (!i915_vma_is_bound(vma, I915_VMA_BIND_MASK))
@@ -184,41 +226,19 @@ int i915_gem_object_unbind(struct drm_i915_gem_object *obj,
 			goto put_vma;
 		}
 
-		/*
-		 * As some machines use ACPI to handle runtime-resume
-		 * callbacks, and ACPI is quite kmalloc happy, we cannot resume
-		 * beneath the vm->mutex as they are required by the shrinker.
-		 * Ergo, we wake the device up first just in case.
-		 */
-		if (!wakeref && i915_vma_is_ggtt(vma))
-			wakeref = intel_runtime_pm_get(rpm);
-
-		if (i915_vma_is_persistent(vma)) {
-			ret = __i915_gem_object_lock_to_evict(vm->root_obj, ww);
-			switch (ret) {
-			case 0:
-				unlock = vm->root_obj;
-				break;
-
-			case -EALREADY:
-				if (flags & I915_GEM_OBJECT_UNBIND_ACTIVE)
-					break;
-				fallthrough;
-			default:
-				goto put_vma;
-			}
-		}
-
-		ret = -EAGAIN;
-		if (mutex_trylock(&vm->mutex)) {
-			if (flags & I915_GEM_OBJECT_UNBIND_ACTIVE ||
-			    !i915_vma_is_active(vma))
+		if (i915_vma_is_persistent(vma) &&
+		    !i915->params.enable_non_private_objects) {
+			ret = i915_vma_unbind_persistent(vma, ww, flags);
+		} else if (flags & I915_GEM_OBJECT_UNBIND_VM_TRYLOCK) {
+			if (mutex_trylock(&vma->vm->mutex)) {
 				ret = __i915_vma_unbind(vma);
-			mutex_unlock(&vm->mutex);
+				mutex_unlock(&vma->vm->mutex);
+			} else {
+				ret = -EBUSY;
+			}
+		} else {
+			ret = i915_vma_unbind(vma);
 		}
-
-		if (unlock)
-			i915_gem_object_unlock(unlock);
 put_vma:
 		__i915_vma_put(vma);
 close_vm:
diff --git a/drivers/gpu/drm/i915/i915_gem_ww.c b/drivers/gpu/drm/i915/i915_gem_ww.c
index 5d840fa4d3f1..4903463f986c 100644
--- a/drivers/gpu/drm/i915/i915_gem_ww.c
+++ b/drivers/gpu/drm/i915/i915_gem_ww.c
@@ -14,6 +14,7 @@ void i915_gem_ww_ctx_init(struct i915_gem_ww_ctx *ww, bool intr)
 {
 	ww_acquire_init(&ww->ctx, &reservation_ww_class);
 	INIT_LIST_HEAD(&ww->obj_list);
+	INIT_LIST_HEAD(&ww->eviction_list);
 
 	ww->region.mem = NULL;
 	ww->region.next = NULL;
@@ -95,11 +96,18 @@ static void put_obj_list(struct list_head *list)
 	INIT_LIST_HEAD(list);
 }
 
+void i915_gem_ww_ctx_unlock_evictions(struct i915_gem_ww_ctx *ww)
+{
+	put_obj_list(&ww->eviction_list);
+}
+
 static void i915_gem_ww_ctx_unlock_all(struct i915_gem_ww_ctx *ww)
 {
 	i915_gem_ww_ctx_remove_regions(ww);
 
 	put_obj_list(&ww->obj_list);
+
+	i915_gem_ww_ctx_unlock_evictions(ww);
 }
 
 void i915_gem_ww_unlock_single(struct drm_i915_gem_object *obj)
@@ -151,21 +159,3 @@ int __must_check i915_gem_ww_ctx_backoff(struct i915_gem_ww_ctx *ww)
 	ww->contended = NULL;
 	return ret;
 }
-
-int
-__i915_gem_object_lock_to_evict(struct drm_i915_gem_object *obj,
-				struct i915_gem_ww_ctx *ww)
-{
-	int err;
-
-	if (ww)
-		err = dma_resv_lock_interruptible(obj->base.resv, &ww->ctx);
-	else
-		err = dma_resv_trylock(obj->base.resv) ? 0 : -EBUSY;
-	if (err == -EDEADLK) {
-		ww->contended_evict = true;
-		ww->contended = i915_gem_object_get(obj);
-	}
-
-	return err;
-}
diff --git a/drivers/gpu/drm/i915/i915_gem_ww.h b/drivers/gpu/drm/i915/i915_gem_ww.h
index 9c4f53e23259..fcb0301ba0c8 100644
--- a/drivers/gpu/drm/i915/i915_gem_ww.h
+++ b/drivers/gpu/drm/i915/i915_gem_ww.h
@@ -12,6 +12,7 @@ struct intel_memory_region;
 struct i915_gem_ww_ctx {
 	struct ww_acquire_ctx ctx;
 	struct list_head obj_list;
+	struct list_head eviction_list;
 	struct i915_gem_ww_region {
 		struct list_head link;
 		struct list_head locked;
@@ -28,10 +29,7 @@ void i915_gem_ww_ctx_init(struct i915_gem_ww_ctx *ctx, bool intr);
 void i915_gem_ww_ctx_fini(struct i915_gem_ww_ctx *ctx);
 int __must_check i915_gem_ww_ctx_backoff(struct i915_gem_ww_ctx *ctx);
 void i915_gem_ww_unlock_single(struct drm_i915_gem_object *obj);
-
-int
-__i915_gem_object_lock_to_evict(struct drm_i915_gem_object *obj,
-				struct i915_gem_ww_ctx *ww);
+void i915_gem_ww_ctx_unlock_evictions(struct i915_gem_ww_ctx *ww);
 
 /* Internal functions used by the inlines! Don't use. */
 static inline int __i915_gem_ww_fini(struct i915_gem_ww_ctx *ww, int err)
diff --git a/drivers/gpu/drm/i915/i915_params.c b/drivers/gpu/drm/i915/i915_params.c
index ac245e2acede..98b64a602661 100644
--- a/drivers/gpu/drm/i915/i915_params.c
+++ b/drivers/gpu/drm/i915/i915_params.c
@@ -248,6 +248,21 @@ i915_param_named_unsafe(enable_eviction, uint, 0600,
 	"0=disabled, 1=memcpy based only, 2=blt based only, "
 	"3=blt based but fallsback to memcpy based [default])");
 
+/*
+ * In execbuff path, we should iterate over all non-private (shared) objects of
+ * the VM to take the dma_resv lock. But this causes a performance degradation
+ * as execbuff latency will be O(n) where 'n' is the number of non_private
+ * objects. Hence adding this enable_non_private_objects module param (default
+ * fasle) to control this feature.
+ *
+ * Having this module param and disabling it by default is an ugly performance
+ * hack so that UMDs can continue with same performance as before, until they
+ * measure (and improve) the performance with proper handling of non-private
+ * objects is enabled.
+ */
+i915_param_named_unsafe(enable_non_private_objects, bool, 0400,
+			"Enable non-private objects handling in execbuff path");
+
 i915_param_named(max_vfs, uint, 0400,
 	"Limit number of virtual functions to allocate. "
 	"(0 = no VFs [default]; N = allow up to N VFs)");
diff --git a/drivers/gpu/drm/i915/i915_params.h b/drivers/gpu/drm/i915/i915_params.h
index ef578b7df134..059dfe8546b6 100644
--- a/drivers/gpu/drm/i915/i915_params.h
+++ b/drivers/gpu/drm/i915/i915_params.h
@@ -112,6 +112,7 @@ struct drm_printer;
 	param(bool, nuclear_pageflip, false, 0400) \
 	param(bool, enable_dp_mst, true, 0600) \
 	param(bool, enable_gvt, false, IS_ENABLED(CONFIG_DRM_I915_GVT) ? 0400 : 0) \
+	param(bool, enable_non_private_objects, false, 0400) \
 	param(bool, enable_mem_fence, false, 0400) \
 	param(bool, ulls_bcs0_pm_wa, true, 0600) \
 	param(int, force_driver_flr, -1, 0400)
diff --git a/drivers/gpu/drm/i915/i915_sw_fence_work.c b/drivers/gpu/drm/i915/i915_sw_fence_work.c
index 10ea6e9aa642..c9bca81c3bcc 100644
--- a/drivers/gpu/drm/i915/i915_sw_fence_work.c
+++ b/drivers/gpu/drm/i915/i915_sw_fence_work.c
@@ -4,8 +4,6 @@
  * Copyright © 2019 Intel Corporation
  */
 
-#include <linux/sched/signal.h>
-
 #include "i915_sw_fence_work.h"
 
 static void fence_complete(struct dma_fence_work *f)
diff --git a/drivers/gpu/drm/i915/i915_vma.c b/drivers/gpu/drm/i915/i915_vma.c
index 75cac269c370..de3177b6216f 100644
--- a/drivers/gpu/drm/i915/i915_vma.c
+++ b/drivers/gpu/drm/i915/i915_vma.c
@@ -267,7 +267,9 @@ vma_create(struct drm_i915_gem_object *obj,
 	spin_lock_init(&vma->metadata_lock);
 	INIT_LIST_HEAD(&vma->metadata_list);
 	INIT_LIST_HEAD(&vma->vm_bind_link);
+	INIT_LIST_HEAD(&vma->non_priv_vm_bind_link);
 	INIT_LIST_HEAD(&vma->vm_capture_link);
+	INIT_LIST_HEAD(&vma->vm_rebind_link);
 	return vma;
 
 err_unlock:
@@ -1738,11 +1740,9 @@ int _i915_vma_move_to_active(struct i915_vma *vma,
 
 void __i915_vma_evict(struct i915_vma *vma)
 {
-	struct i915_address_space *vm = vma->vm;
-
 	GEM_BUG_ON(i915_vma_is_pinned(vma));
 
-	i915_debugger_vma_evict(vm->client, vma);
+	i915_debugger_vma_evict(vma->vm->client, vma);
 
 	if (i915_vma_is_map_and_fenceable(vma)) {
 		/* Force a pagefault for domain tracking on next user access */
@@ -1774,24 +1774,16 @@ void __i915_vma_evict(struct i915_vma *vma)
 	GEM_BUG_ON(vma->fence);
 	GEM_BUG_ON(i915_vma_has_userfault(vma));
 
-	if (likely(atomic_read(&vm->open))) {
+	if (likely(atomic_read(&vma->vm->open))) {
 		trace_i915_vma_unbind(vma);
-		vma->ops->unbind_vma(vm, vma);
-
-		if (!list_empty(&vma->vm_bind_link)) {
-			GEM_BUG_ON(!i915_vma_is_persistent(vma));
-			assert_object_held(vm->root_obj);
-			list_move_tail(&vma->vm_bind_link, &vm->vm_bind_list);
-		}
+		vma->ops->unbind_vma(vma->vm, vma);
 	}
 	atomic_and(~(I915_VMA_BIND_MASK | I915_VMA_ERROR | I915_VMA_GGTT_WRITE),
 		   &vma->flags);
 
-	if (!i915_vm_page_fault_enabled(vm) ||
-	    i915_vma_is_purged(vma) ||
-	    !i915_vma_is_persistent(vma))
+	if(!i915_vm_page_fault_enabled(vma->vm) || i915_vma_is_purged(vma) ||
+	   !i915_vma_is_persistent(vma))
 		i915_vma_detach(vma);
-
 	vma_unbind_pages(vma);
 }
 
@@ -1819,12 +1811,29 @@ int __i915_vma_unbind(struct i915_vma *vma)
 	if (ret)
 		return ret;
 
+	GEM_BUG_ON(i915_vma_is_active(vma));
 	__i915_vma_evict(vma);
 
-	if (!i915_vm_page_fault_enabled(vm) || i915_vma_is_purged(vma) ||
-	    !i915_vma_is_persistent(vma))
+	if(!i915_vm_page_fault_enabled(vm) || i915_vma_is_purged(vma) ||
+	   !i915_vma_is_persistent(vma))
 		drm_mm_remove_node(&vma->node);
 
+	if (i915_vma_is_persistent(vma)) {
+		spin_lock(&vm->vm_rebind_lock);
+
+		/*
+		 * Confirm this vma is still alive after acquiring the spinlock
+		 * to serialize with i915_gem_vm_bind_remove(). Otherwise, we
+		 * may re-add the vma onto the rebind list /after/ the vma has
+		 * already been marked as freed.
+		 */
+		if (list_empty(&vma->vm_rebind_link))
+			list_add_tail(&vma->vm_rebind_link,
+				      &vm->vm_rebind_list);
+
+		spin_unlock(&vm->vm_rebind_lock);
+	}
+
 	return 0;
 }
 
@@ -1886,24 +1895,30 @@ int i915_vma_prefetch(struct i915_vma *vma, struct intel_memory_region *mem)
 	if (i915_gem_object_is_userptr(vma->obj))
 		return -EINVAL;
 
-	for_i915_gem_ww(&ww, err, true) {
-		err = i915_gem_object_lock(vma->vm->root_obj, &ww);
-		if (err)
-			continue;
+	i915_gem_ww_ctx_init(&ww, true);
 
-		err = i915_gem_object_lock(vma->obj, &ww);
-		if (err)
-			continue;
+retry:
+	err = i915_gem_object_lock(vma->obj, &ww);
+	if (err)
+		goto err_ww;
 
-		if (vma->obj->mm.region.mem->id != mem->id)
-			err = i915_gem_object_migrate_region(vma->obj, &ww, &mem, 1);
-		if (err)
-			continue;
+	if (vma->obj->mm.region.mem->id != mem->id)
+		err = i915_gem_object_migrate_region(vma->obj, &ww, &mem, 1);
+	if (err)
+		goto err_ww;
+
+	if (i915_vma_is_bound(vma, PIN_RESIDENT))
+		goto err_ww;
 
-		if (!i915_vma_is_bound(vma, PIN_RESIDENT))
-			err = i915_vma_bind(vma, &ww);
+	err = i915_vma_bind(vma, &ww);
+err_ww:
+	if (err == -EDEADLK) {
+		err = i915_gem_ww_ctx_backoff(&ww);
+		if (!err)
+			goto retry;
 	}
 
+	i915_gem_ww_ctx_fini(&ww);
 	return err;
 }
 
diff --git a/drivers/gpu/drm/i915/i915_vma_types.h b/drivers/gpu/drm/i915/i915_vma_types.h
index 77905a50b1f1..bf675d1f060f 100644
--- a/drivers/gpu/drm/i915/i915_vma_types.h
+++ b/drivers/gpu/drm/i915/i915_vma_types.h
@@ -278,7 +278,6 @@ struct i915_vma {
 
 #define I915_VMA_PERSISTENT_BIT	19
 #define I915_VMA_PURGED_BIT	20
-#define I915_VMA_HAS_LUT_BIT	21
 
 	struct i915_active active;
 
@@ -300,13 +299,18 @@ struct i915_vma {
 	struct list_head vm_link;
 
 	struct list_head vm_bind_link; /* Link in persistent VMA list */
+	/* Link in non-private persistent VMA list */
+	struct list_head non_priv_vm_bind_link;
 	struct list_head vm_capture_link; /* Link in captureable VMA list */
+	struct list_head vm_rebind_link; /* Link in vm_rebind_list */
 	struct i915_sw_fence *bind_fence;
 	/* (segmented BO) walk adjacent VMAs at unbind or during capture_vma */
 	struct i915_vma *adjacent_next;
 
 	/** Interval tree structures for persistent vma */
 	struct rb_node rb;
+	u64 start;
+	u64 last;
 	u64 __subtree_last;
 
 	struct list_head obj_link; /* Link in the object's VMA list */
diff --git a/drivers/gpu/drm/i915/intel_memory_region.c b/drivers/gpu/drm/i915/intel_memory_region.c
index 259d0c30b05f..46e03d21480a 100644
--- a/drivers/gpu/drm/i915/intel_memory_region.c
+++ b/drivers/gpu/drm/i915/intel_memory_region.c
@@ -353,6 +353,20 @@ int intel_memory_region_add_to_ww_evictions(struct intel_memory_region *mem,
 	return 0;
 }
 
+static int __i915_gem_object_lock_to_evict(struct drm_i915_gem_object *obj,
+					   struct i915_gem_ww_ctx *ww)
+{
+	int err;
+
+	err = dma_resv_lock_interruptible(obj->base.resv, &ww->ctx);
+	if (err == -EDEADLK) {
+		ww->contended_evict = true;
+		ww->contended = i915_gem_object_get(obj);
+	}
+
+	return err;
+}
+
 static bool i915_gem_object_allows_eviction(struct drm_i915_gem_object *obj)
 {
 	/* Only evict user lmem only objects if overcommit is enabled */
@@ -472,9 +486,14 @@ static int intel_memory_region_evict(struct intel_memory_region *mem,
 			goto put;
 		}
 
-		err = __i915_gem_object_lock_to_evict(obj, ww);
-		if (err)
-			goto put;
+		if (ww) {
+			err = __i915_gem_object_lock_to_evict(obj, ww);
+			if (err)
+				goto put;
+		} else {
+			if (!i915_gem_object_trylock(obj))
+				goto put;
+		}
 
 		if (!i915_gem_object_has_pages(obj))
 			goto unlock;
@@ -486,15 +505,12 @@ static int intel_memory_region_evict(struct intel_memory_region *mem,
 
 		GEM_TRACE("%s:{ target:%pa, found:%pa, evicting:%zu, remaining timeout:%ld }\n",
 			  mem->name, &target, &found, obj->base.size, timeout);
-		err = i915_gem_object_unbind(obj, ww, 0);
+		err = i915_gem_object_unbind(obj, ww, I915_GEM_OBJECT_UNBIND_ACTIVE);
 		if (err == 0)
 			err = __i915_gem_object_put_pages(obj);
 		if (err == 0)
 			/* conservative estimate of reclaimed pages */
 			found += obj->base.size;
-		else
-			/* repeat this phase to resolve transient contention */
-			busy = true;
 
 unlock:
 		i915_gem_object_unlock(obj);
diff --git a/drivers/gpu/drm/i915/selftests/mock_gtt.c b/drivers/gpu/drm/i915/selftests/mock_gtt.c
index 999cde738d44..3bd4c5aff0db 100644
--- a/drivers/gpu/drm/i915/selftests/mock_gtt.c
+++ b/drivers/gpu/drm/i915/selftests/mock_gtt.c
@@ -65,7 +65,6 @@ static void mock_clear_range(struct i915_address_space *vm,
 struct i915_ppgtt *mock_ppgtt(struct drm_i915_private *i915, const char *name)
 {
 	struct i915_ppgtt *ppgtt;
-	int err;
 
 	ppgtt = kzalloc(sizeof(*ppgtt), GFP_KERNEL);
 	if (!ppgtt)
@@ -76,11 +75,7 @@ struct i915_ppgtt *mock_ppgtt(struct drm_i915_private *i915, const char *name)
 	ppgtt->vm.total = round_down(U64_MAX, PAGE_SIZE);
 	ppgtt->vm.dma = i915->drm.dev;
 
-	err = i915_address_space_init(&ppgtt->vm, VM_CLASS_PPGTT);
-	if (err) {
-		kfree(ppgtt);
-		return ERR_PTR(err);
-	}
+	i915_address_space_init(&ppgtt->vm, VM_CLASS_PPGTT);
 
 	ppgtt->vm.alloc_pt_dma = alloc_pt_dma;
 	ppgtt->vm.alloc_scratch_dma = alloc_pt_dma;
-- 
2.40.0

