From 9011dbbc4f20877707cd27932667a42a23d15382 Mon Sep 17 00:00:00 2001
From: Lakshmishree C <Lakshmishree C lakshmishree.c@intel.com>
Date: Fri, 13 Sep 2019 11:42:32 +0530
Subject: [PATCH] Fix for nn-hal Klocwork issues

---
 intel_nn_hal/PreparedModel.cpp           | 16 ++++++++--------
 intel_nn_hal/PreparedModel.h             |  2 +-
 intel_nn_hal/graphAPI/IRLayers.h         |  2 +-
 intel_nn_hal/graphTests/helpers-test.hpp |  4 ++--
 intel_nn_hal/graphTests/main.cpp         | 10 +++++++---
 5 files changed, 19 insertions(+), 15 deletions(-)

diff --git a/intel_nn_hal/PreparedModel.cpp b/intel_nn_hal/PreparedModel.cpp
index c06b499..2a53663 100644
--- a/intel_nn_hal/PreparedModel.cpp
+++ b/intel_nn_hal/PreparedModel.cpp
@@ -1970,10 +1970,10 @@ bool PreparedModel::operationAveragePool2D(const Operation& operation) {
     auto input = getPort(operation.inputs[0]);
     const auto indims = input->getTensorDesc().getDims();
 
-    Point2D pad_start;
-    Point2D pad_end;
-    Point2D stride;
-    Point2D kernel;
+    Point2D pad_start = {0, 0};
+    Point2D pad_end = {0, 0};
+    Point2D stride = {0, 0};
+    Point2D kernel = {0, 0};
     std::string padType;
     int fusion_index = -1;
 
@@ -2067,10 +2067,10 @@ bool PreparedModel::operationMaxPool2D(const Operation& operation) {
     auto input = getPort(operation.inputs[0]);
     const auto indims = input->getTensorDesc().getDims();
 
-    Point2D pad_start;
-    Point2D pad_end;
-    Point2D stride;
-    Point2D kernel;
+    Point2D pad_start = {0, 0};
+    Point2D pad_end = {0, 0};
+    Point2D stride = {0, 0};
+    Point2D kernel = {0, 0};
     std::string padType;
     int fusion_index = -1;
 
diff --git a/intel_nn_hal/PreparedModel.h b/intel_nn_hal/PreparedModel.h
index 24f3562..cbb17de 100644
--- a/intel_nn_hal/PreparedModel.h
+++ b/intel_nn_hal/PreparedModel.h
@@ -106,7 +106,7 @@ bool setRunTimePoolInfosFromHidlMemories(std::vector<RunTimePoolInfo>* poolInfos
 class PreparedModel : public IPreparedModel {
 public:
     PreparedModel(const Model& model)
-        : mTargetDevice(TargetDevice::eMYRIAD), mModel(model), mNet("nnNet"), enginePtr(nullptr) {
+        : mTargetDevice(TargetDevice::eMYRIAD), mModel(model), mNet("nnNet"), enginePtr(nullptr), mPadreq(false) {
         g_layer_precision = InferenceEngine::Precision::FP16;
     }
 
diff --git a/intel_nn_hal/graphAPI/IRLayers.h b/intel_nn_hal/graphAPI/IRLayers.h
index 10fcdae..3b309e3 100644
--- a/intel_nn_hal/graphAPI/IRLayers.h
+++ b/intel_nn_hal/graphAPI/IRLayers.h
@@ -399,7 +399,7 @@ inline OutputPort Convolution(const OutputPort &src, const ConvolutionParams &pr
         size_t IH = inDims[2];
         size_t IW = inDims[3];
         size_t KH = 0, KW = 0;
-        float OH_temp, OW_temp;
+        float OH_temp = 1.f, OW_temp = 1.f;
 
         if (ret->_dilation[InferenceEngine::Y_AXIS])
             KH = (ret->_kernel[InferenceEngine::Y_AXIS] - 1) *
diff --git a/intel_nn_hal/graphTests/helpers-test.hpp b/intel_nn_hal/graphTests/helpers-test.hpp
index 4ed3a38..477754b 100644
--- a/intel_nn_hal/graphTests/helpers-test.hpp
+++ b/intel_nn_hal/graphTests/helpers-test.hpp
@@ -244,11 +244,11 @@ class ExecuteNetwork {
     OutputsDataMap outputInfo = {};
     IInferRequest::Ptr req;
     InferRequest inferRequest;
-    ResponseDesc resp;
+    ResponseDesc resp = {};
 
 public:
     ExecuteNetwork() : network(nullptr) {}
-    ExecuteNetwork(IRDocument &doc, TargetDevice target = TargetDevice::eCPU) : network(nullptr) {
+    ExecuteNetwork(IRDocument &doc, TargetDevice target = TargetDevice::eCPU) : network(nullptr), executable_network(), req(nullptr), inferRequest() {
         InferenceEngine::PluginDispatcher dispatcher(
             {"/vendor/lib64", "/vendor/lib", "/system/lib64", "/system/lib", "", "./"});
         enginePtr = dispatcher.getSuitablePlugin(target);
diff --git a/intel_nn_hal/graphTests/main.cpp b/intel_nn_hal/graphTests/main.cpp
index 7c542af..2e52cfe 100644
--- a/intel_nn_hal/graphTests/main.cpp
+++ b/intel_nn_hal/graphTests/main.cpp
@@ -146,12 +146,16 @@ bool testAlexNet() {
 
     // try it out
     try {
+        Blob::Ptr in = nullptr;
 #ifdef ENABLE_MYRIAD
-        auto in = readBlobFromFile<short>("AlexNet-bins/input.bin", {1, 3, 227, 227},
+        in = readBlobFromFile<short>("AlexNet-bins/input.bin", {1, 3, 227, 227},
                                           InferenceEngine::NCHW);
 #elif ENABLE_MKLDNN
-        auto in = readBlobFromFile<float>("AlexNet-bins/input.bin", {1, 3, 227, 227},
-                                          InferenceEngine::NCHW);
+        in = readBlobFromFile<float>("AlexNet-bins/input.bin", {1, 3, 227, 227},
+                                          InferenceEngine::NCHW);      
+#else
+        std::cerr << "Device Type not specified.";
+        return false;
 #endif
         /*
                         InfEng eng(TargetDevice::eMYRIAD);
-- 
2.17.1

